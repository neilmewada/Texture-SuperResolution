{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28796dc-582a-4e2a-8916-1eb773c5a86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from common import *\n",
    "from dataset import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2061d455-6ead-4b90-a5fc-35353756e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 100000\n",
    "batch_size = 128\n",
    "scale = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "056c978c-2d50-4be8-8c19-62a72d8dcfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Projects\\\\Texture-SuperResolution/ambient-cg-images'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATH = f\"{os.getcwd()}/ambient-cg-images\"\n",
    "DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fc9d6b-bd17-490f-bf72-b3c676d0ac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 images\n"
     ]
    }
   ],
   "source": [
    "def _list_images(root: str, exts=(\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\")) -> List[str]:\n",
    "    return [os.path.join(root, f) for f in os.listdir(root) if f.lower().endswith(exts)]\n",
    "\n",
    "def _ensure_divisible_by(img: Image.Image, s: int) -> Image.Image:\n",
    "    w, h = img.size\n",
    "    w2, h2 = (w // s) * s, (h // s) * s\n",
    "    return img if (w2, h2) == (w, h) else img.crop((0, 0, w2, h2))\n",
    "\n",
    "def _to_tensor(img: Image.Image) -> torch.Tensor:\n",
    "    arr = np.asarray(img).astype(np.float32) / 255.0  # H,W,3\n",
    "    arr = np.transpose(arr, (2, 0, 1))               # 3,H,W\n",
    "    return torch.from_numpy(arr)\n",
    "\n",
    "class AmbientCG_FSRCNN_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    FSRCNN dataset producing (LR, HR, name)\n",
    "    - ORIGINAL (≈2K) --(crop to divisible by 2*scale)-->  downscale by 2  --> HR (≈1K)\n",
    "    - LR = downsample(HR) by `scale`\n",
    "    - mode='patch': random HR patch (size divisible by scale) from HR, then LR from that patch\n",
    "    - mode='full' : full HR image\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_list: List[str],\n",
    "        scale: int = 4,\n",
    "        mode: str = \"patch\",\n",
    "        hr_patch_size: int = 128,\n",
    "        patches_per_image: int = 16,\n",
    "        augment: bool = True,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        assert scale in (2,3,4)\n",
    "        assert mode in (\"patch\", \"full\")\n",
    "        if mode == \"patch\":\n",
    "            assert hr_patch_size % scale == 0, \"hr_patch_size must be divisible by scale\"\n",
    "        self.files = file_list\n",
    "        self.scale = scale\n",
    "        self.mode = mode\n",
    "        self.hr_patch_size = hr_patch_size\n",
    "        self.patches_per_image = patches_per_image\n",
    "        self.augment = augment\n",
    "\n",
    "        # deterministic index map for patch mode\n",
    "        self.idx_map: List[Tuple[int,int]] = []\n",
    "        if mode == \"patch\":\n",
    "            for i in range(len(self.files)):\n",
    "                for k in range(self.patches_per_image):\n",
    "                    self.idx_map.append((i, k))\n",
    "            random.Random(seed).shuffle(self.idx_map)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_map) if self.mode == \"patch\" else len(self.files)\n",
    "\n",
    "    @staticmethod\n",
    "    def _rand_patch(img: Image.Image, size: int) -> Image.Image:\n",
    "        w, h = img.size\n",
    "        if w < size or h < size:\n",
    "            img = img.resize((max(w, size), max(h, size)), Image.Resampling.BICUBIC)\n",
    "            w, h = img.size\n",
    "        x = random.randint(0, w - size); y = random.randint(0, h - size)\n",
    "        return img.crop((x, y, x + size, y + size))\n",
    "\n",
    "    @staticmethod\n",
    "    def _augment_flip_rot(img: Image.Image) -> Image.Image:\n",
    "        if random.random() < 0.5: img = img.transpose(Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "        if random.random() < 0.5: img = img.transpose(Image.Transpose.FLIP_TOP_BOTTOM)\n",
    "        k = random.randint(0,3)\n",
    "        if k: img = img.rotate(90*k, expand=False)\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ---- 1) Load ORIGINAL (~2K) and ensure divisibility by 2*scale ----\n",
    "        if self.mode == \"patch\":\n",
    "            img_idx, _ = self.idx_map[idx]\n",
    "            path = self.files[img_idx]\n",
    "        else:\n",
    "            path = self.files[idx]\n",
    "\n",
    "        orig = Image.open(path).convert(\"RGB\")\n",
    "        orig = _ensure_divisible_by(orig, 2 * self.scale)  # so HR (orig/2) is divisible by scale\n",
    "\n",
    "        # ---- 2) Downscale ORIGINAL by 2 to form HR (~1K) ----\n",
    "        ow, oh = orig.size\n",
    "        hr_size = (ow // 2, oh // 2)\n",
    "        hr = orig.resize(hr_size, Image.Resampling.BICUBIC)\n",
    "\n",
    "        # (safety) ensure HR divisible by scale\n",
    "        hr = _ensure_divisible_by(hr, self.scale)\n",
    "\n",
    "        # ---- 3) Patch/augment ON HR (not on the original) ----\n",
    "        if self.mode == \"patch\":\n",
    "            hr = self._rand_patch(hr, self.hr_patch_size)  # size divisible by scale\n",
    "            if self.augment:\n",
    "                hr = self._augment_flip_rot(hr)\n",
    "\n",
    "        # ---- 4) Create LR by downsampling HR by `scale` ----\n",
    "        hr_w, hr_h = hr.size\n",
    "        lr = hr.resize((hr_w // self.scale, hr_h // self.scale), Image.Resampling.BICUBIC)\n",
    "\n",
    "        return _to_tensor(lr), _to_tensor(hr), os.path.basename(path)                   \n",
    "\n",
    "all_files = _list_images(DATASET_PATH)\n",
    "all_files = all_files[0:200]\n",
    "print(f\"Found {len(all_files)} images\")\n",
    "\n",
    "# train/val split (disjoint by file)\n",
    "train_files, val_files = train_test_split(all_files, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595bbc70-0bc3-4ba5-978e-66bcf86a3551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Projects\\\\Texture-SuperResolution/ambient-cg-images\\\\Asphalt004_2K-JPG_Color.jpg'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d9491b-9851-455c-851f-3adbd43248fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = AmbientCG_FSRCNN_Dataset(\n",
    "    file_list=train_files,\n",
    "    scale=scale,\n",
    "    mode=\"patch\",\n",
    "    hr_patch_size=128,       # divisible by SCALE\n",
    "    patches_per_image=16,\n",
    "    augment=True,\n",
    ")\n",
    "\n",
    "val_ds = AmbientCG_FSRCNN_Dataset(\n",
    "    file_list=val_files,\n",
    "    scale=scale,\n",
    "    mode=\"full\",             # full frames for validation; batch_size must be 1\n",
    "    augment=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14273057-f2a8-4264-bd97-fee2c102010e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2560, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a848a560-7430-4ff6-baa3-77acd0545497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "SCALE        = 4                                # MUST match your dataset/model\n",
    "BATCH_SIZE   = 16\n",
    "NUM_EPOCHS   = 40\n",
    "LR           = 1e-4\n",
    "STEP_EVERY   = 15\n",
    "GAMMA        = 0.5\n",
    "NUM_WORKERS  = 0                                # Windows/Jupyter-safe\n",
    "PIN_MEMORY   = False\n",
    "BORDER_CROP  = 4                                # shave borders for PSNR(Y)\n",
    "CKPT_PATH    = f\"fsrcnn_x{SCALE}_best.pth\"\n",
    "PRETRAINED   = f\"pretrained_fsrcnn_x{SCALE}.pth\" # put the repo weights here if you have them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef9bfaf1-24b9-4466-92a6-a51dd99b2776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "No pretrained weights provided; training from scratch.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neelr\\AppData\\Local\\Temp\\ipykernel_11908\\574368079.py:58: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "C:\\Users\\neelr\\AppData\\Local\\Temp\\ipykernel_11908\\574368079.py:76: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=use_amp):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/40 | train_loss=0.110317 | val_loss=0.048822 | PSNR(Y) srcnn=26.17 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 290.6s\n",
      "------------------------------------------------------------\n",
      "Epoch 002/40 | train_loss=0.052258 | val_loss=0.040825 | PSNR(Y) srcnn=27.22 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 294.9s\n",
      "------------------------------------------------------------\n",
      "Epoch 003/40 | train_loss=0.044468 | val_loss=0.039207 | PSNR(Y) srcnn=27.63 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 301.3s\n",
      "------------------------------------------------------------\n",
      "Epoch 004/40 | train_loss=0.042275 | val_loss=0.036197 | PSNR(Y) srcnn=28.10 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 941.2s\n",
      "------------------------------------------------------------\n",
      "Epoch 005/40 | train_loss=0.040292 | val_loss=0.034240 | PSNR(Y) srcnn=28.51 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 320.9s\n",
      "------------------------------------------------------------\n",
      "Epoch 006/40 | train_loss=0.038307 | val_loss=0.032124 | PSNR(Y) srcnn=28.91 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 317.5s\n",
      "------------------------------------------------------------\n",
      "Epoch 007/40 | train_loss=0.036950 | val_loss=0.031164 | PSNR(Y) srcnn=29.09 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 314.0s\n",
      "------------------------------------------------------------\n",
      "Epoch 008/40 | train_loss=0.035428 | val_loss=0.030475 | PSNR(Y) srcnn=29.26 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 313.7s\n",
      "------------------------------------------------------------\n",
      "Epoch 009/40 | train_loss=0.034839 | val_loss=0.029866 | PSNR(Y) srcnn=29.38 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 314.2s\n",
      "------------------------------------------------------------\n",
      "Epoch 010/40 | train_loss=0.033984 | val_loss=0.029649 | PSNR(Y) srcnn=29.41 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 312.1s\n",
      "------------------------------------------------------------\n",
      "Epoch 011/40 | train_loss=0.033450 | val_loss=0.029577 | PSNR(Y) srcnn=29.46 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 313.6s\n",
      "------------------------------------------------------------\n",
      "Epoch 012/40 | train_loss=0.032821 | val_loss=0.028853 | PSNR(Y) srcnn=29.63 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 313.0s\n",
      "------------------------------------------------------------\n",
      "Epoch 013/40 | train_loss=0.032305 | val_loss=0.028578 | PSNR(Y) srcnn=29.69 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 312.7s\n",
      "------------------------------------------------------------\n",
      "Epoch 014/40 | train_loss=0.031892 | val_loss=0.028466 | PSNR(Y) srcnn=29.69 dB | PSNR(Y) bic=30.72 dB | lr=1.0e-04 | 318.5s\n",
      "------------------------------------------------------------\n",
      "Epoch 015/40 | train_loss=0.031617 | val_loss=0.028007 | PSNR(Y) srcnn=29.80 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 322.0s\n",
      "------------------------------------------------------------\n",
      "Epoch 016/40 | train_loss=0.031181 | val_loss=0.027868 | PSNR(Y) srcnn=29.84 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 313.3s\n",
      "------------------------------------------------------------\n",
      "Epoch 017/40 | train_loss=0.031066 | val_loss=0.027708 | PSNR(Y) srcnn=29.89 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 315.0s\n",
      "------------------------------------------------------------\n",
      "Epoch 018/40 | train_loss=0.030900 | val_loss=0.027964 | PSNR(Y) srcnn=29.73 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 312.5s\n",
      "------------------------------------------------------------\n",
      "Epoch 019/40 | train_loss=0.030746 | val_loss=0.027692 | PSNR(Y) srcnn=29.86 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 315.1s\n",
      "------------------------------------------------------------\n",
      "Epoch 020/40 | train_loss=0.030777 | val_loss=0.027629 | PSNR(Y) srcnn=29.87 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 313.0s\n",
      "------------------------------------------------------------\n",
      "Epoch 021/40 | train_loss=0.030461 | val_loss=0.027351 | PSNR(Y) srcnn=29.99 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 317.4s\n",
      "------------------------------------------------------------\n",
      "Epoch 022/40 | train_loss=0.030293 | val_loss=0.027249 | PSNR(Y) srcnn=30.01 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 317.5s\n",
      "------------------------------------------------------------\n",
      "Epoch 023/40 | train_loss=0.030162 | val_loss=0.027186 | PSNR(Y) srcnn=30.05 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 315.8s\n",
      "------------------------------------------------------------\n",
      "Epoch 024/40 | train_loss=0.030071 | val_loss=0.027079 | PSNR(Y) srcnn=30.04 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 314.5s\n",
      "------------------------------------------------------------\n",
      "Epoch 025/40 | train_loss=0.029898 | val_loss=0.026993 | PSNR(Y) srcnn=30.07 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 314.3s\n",
      "------------------------------------------------------------\n",
      "Epoch 026/40 | train_loss=0.029772 | val_loss=0.026898 | PSNR(Y) srcnn=30.11 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 327.0s\n",
      "------------------------------------------------------------\n",
      "Epoch 027/40 | train_loss=0.029664 | val_loss=0.026818 | PSNR(Y) srcnn=30.13 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 368.1s\n",
      "------------------------------------------------------------\n",
      "Epoch 028/40 | train_loss=0.029592 | val_loss=0.027323 | PSNR(Y) srcnn=29.92 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 329.3s\n",
      "------------------------------------------------------------\n",
      "Epoch 029/40 | train_loss=0.029398 | val_loss=0.026692 | PSNR(Y) srcnn=30.16 dB | PSNR(Y) bic=30.72 dB | lr=5.0e-05 | 336.5s\n",
      "------------------------------------------------------------\n",
      "Epoch 030/40 | train_loss=0.029252 | val_loss=0.026651 | PSNR(Y) srcnn=30.19 dB | PSNR(Y) bic=30.72 dB | lr=2.5e-05 | 354.1s\n",
      "------------------------------------------------------------\n",
      "Epoch 031/40 | train_loss=0.029178 | val_loss=0.026594 | PSNR(Y) srcnn=30.19 dB | PSNR(Y) bic=30.72 dB | lr=2.5e-05 | 314.8s\n",
      "------------------------------------------------------------\n",
      "Epoch 032/40 | train_loss=0.029221 | val_loss=0.026512 | PSNR(Y) srcnn=30.21 dB | PSNR(Y) bic=30.72 dB | lr=2.5e-05 | 317.2s\n",
      "------------------------------------------------------------\n",
      "Epoch 033/40 | train_loss=0.029014 | val_loss=0.026524 | PSNR(Y) srcnn=30.22 dB | PSNR(Y) bic=30.72 dB | lr=2.5e-05 | 323.3s\n",
      "------------------------------------------------------------\n",
      "Epoch 034/40 | train_loss=0.028980 | val_loss=0.026457 | PSNR(Y) srcnn=30.23 dB | PSNR(Y) bic=30.72 dB | lr=2.5e-05 | 315.8s\n",
      "------------------------------------------------------------\n",
      "Epoch 035/40 | train_loss=0.029004 | val_loss=0.026402 | PSNR(Y) srcnn=30.25 dB | PSNR(Y) bic=30.72 dB | lr=2.5e-05 | 315.7s\n",
      "------------------------------------------------------------\n",
      "Epoch 036/40 | train_loss=0.028930 | val_loss=0.026447 | PSNR(Y) srcnn=30.26 dB | PSNR(Y) bic=30.72 dB | lr=2.5e-05 | 311.8s\n",
      "------------------------------------------------------------\n",
      "Epoch 037/40 | train_loss=0.028883 | val_loss=0.026369 | PSNR(Y) srcnn=30.28 dB | PSNR(Y) bic=30.72 dB | lr=2.5e-05 | 311.6s\n",
      "------------------------------------------------------------\n",
      "Epoch 038/40 | train_loss=0.028786 | val_loss=0.026397 | PSNR(Y) srcnn=30.23 dB | PSNR(Y) bic=30.72 dB | lr=2.5e-05 | 312.1s\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m t0 = time.time()\n\u001b[32m     69\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# [B,3,h,w]\u001b[39;49;00m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhr\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# [B,3,H,W]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 85\u001b[39m, in \u001b[36mAmbientCG_FSRCNN_Dataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     83\u001b[39m ow, oh = orig.size\n\u001b[32m     84\u001b[39m hr_size = (ow // \u001b[32m2\u001b[39m, oh // \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m hr = \u001b[43morig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhr_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResampling\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBICUBIC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# (safety) ensure HR divisible by scale\u001b[39;00m\n\u001b[32m     88\u001b[39m hr = _ensure_divisible_by(hr, \u001b[38;5;28mself\u001b[39m.scale)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\PIL\\Image.py:2316\u001b[39m, in \u001b[36mImage.resize\u001b[39m\u001b[34m(self, size, resample, box, reducing_gap)\u001b[39m\n\u001b[32m   2304\u001b[39m         \u001b[38;5;28mself\u001b[39m = (\n\u001b[32m   2305\u001b[39m             \u001b[38;5;28mself\u001b[39m.reduce(factor, box=reduce_box)\n\u001b[32m   2306\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.reduce)\n\u001b[32m   2307\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m Image.reduce(\u001b[38;5;28mself\u001b[39m, factor, box=reduce_box)\n\u001b[32m   2308\u001b[39m         )\n\u001b[32m   2309\u001b[39m         box = (\n\u001b[32m   2310\u001b[39m             (box[\u001b[32m0\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2311\u001b[39m             (box[\u001b[32m1\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2312\u001b[39m             (box[\u001b[32m2\u001b[39m] - reduce_box[\u001b[32m0\u001b[39m]) / factor_x,\n\u001b[32m   2313\u001b[39m             (box[\u001b[32m3\u001b[39m] - reduce_box[\u001b[32m1\u001b[39m]) / factor_y,\n\u001b[32m   2314\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -----------------------\n",
    "# DataLoaders (use your already-created datasets)\n",
    "#   train_ds: patch mode\n",
    "#   val_ds:   full mode (batch 1)\n",
    "# -----------------------\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=1,           shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def rgb_to_y(t: torch.Tensor) -> torch.Tensor:\n",
    "    # [B,3,H,W] -> [B,1,H,W]\n",
    "    r, g, b = t[:,0:1], t[:,1:2], t[:,2:3]\n",
    "    return 0.299*r + 0.587*g + 0.114*b\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_psnr_y(sr: torch.Tensor, hr: torch.Tensor, shave: int = 0) -> float:\n",
    "    # sr, hr: [B,3,H,W] in [0,1]; compute PSNR on Y channel with border crop\n",
    "    sr_y = rgb_to_y(sr)\n",
    "    hr_y = rgb_to_y(hr)\n",
    "    if shave > 0:\n",
    "        sr_y = sr_y[..., shave:-shave, shave:-shave]\n",
    "        hr_y = hr_y[..., shave:-shave, shave:-shave]\n",
    "    mse = torch.mean((sr_y - hr_y) ** 2, dim=(1,2,3))  # per-image\n",
    "    psnr = 20.0 * torch.log10(torch.tensor(1.0, device=sr.device)) - 10.0 * torch.log10(mse.clamp_min(1e-10))\n",
    "    return psnr.mean().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def batch_psnr_y_bicubic(lr: torch.Tensor, hr: torch.Tensor, shave: int = 0, scale: int = SCALE) -> float:\n",
    "    # upsample LR to HR size with bicubic for baseline PSNR(Y)\n",
    "    lr_up = F.interpolate(lr, size=hr.shape[-2:], mode=\"bicubic\", align_corners=False)\n",
    "    return batch_psnr_y(lr_up, hr, shave)\n",
    "\n",
    "# -----------------------\n",
    "# Model / Optim / Loss\n",
    "# -----------------------\n",
    "model = FSRCNN_model(scale=SCALE).to(device)\n",
    "\n",
    "# (Optional) load pretrained repo weights if available\n",
    "if os.path.exists(PRETRAINED):\n",
    "    ckpt = torch.load(PRETRAINED, map_location=device)\n",
    "    state = ckpt.get(\"model_state_dict\", ckpt)  # support plain state_dict too\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(f\"Loaded pretrained: {PRETRAINED} | missing:{len(missing)} unexpected:{len(unexpected)}\")\n",
    "else:\n",
    "    print(\"No pretrained weights provided; training from scratch.\")\n",
    "\n",
    "criterion = nn.L1Loss()                          # L1 works well for textures\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP_EVERY, gamma=GAMMA)\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "best_val_psnr = -1e9\n",
    "\n",
    "# -----------------------\n",
    "# Train / Val loops\n",
    "# -----------------------\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for lr, hr, _ in train_loader:\n",
    "        lr = lr.to(device, non_blocking=True)   # [B,3,h,w]\n",
    "        hr = hr.to(device, non_blocking=True)   # [B,3,H,W]\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            sr = model(lr)                      # -> [B,3,H,W] (FSRCNN upsamples internally)\n",
    "            loss = criterion(sr, hr)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss /= max(1, len(train_loader))\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_psnr_sr  = 0.0\n",
    "    val_psnr_bic = 0.0\n",
    "    with torch.no_grad():\n",
    "        for lr, hr, _ in val_loader:\n",
    "            lr = lr.to(device, non_blocking=True)\n",
    "            hr = hr.to(device, non_blocking=True)\n",
    "            sr = model(lr)\n",
    "\n",
    "            val_loss += criterion(sr, hr).item()\n",
    "            val_psnr_sr  += batch_psnr_y(sr, hr, shave=BORDER_CROP)\n",
    "            val_psnr_bic += batch_psnr_y_bicubic(lr, hr, shave=BORDER_CROP, scale=SCALE)\n",
    "\n",
    "    n_val = max(1, len(val_loader))\n",
    "    val_loss   /= n_val\n",
    "    val_psnr_sr  /= n_val\n",
    "    val_psnr_bic /= n_val\n",
    "\n",
    "    # Save best\n",
    "    if val_psnr_sr > best_val_psnr:\n",
    "        best_val_psnr = val_psnr_sr\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"val_psnr_y\": best_val_psnr,\n",
    "            \"scale\": SCALE\n",
    "        }, CKPT_PATH)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    print(f\"Epoch {epoch:03d}/{NUM_EPOCHS} | \"\n",
    "          f\"train_loss={train_loss:.6f} | \"\n",
    "          f\"val_loss={val_loss:.6f} | \"\n",
    "          f\"PSNR(Y) srcnn={val_psnr_sr:.2f} dB | \"\n",
    "          f\"PSNR(Y) bic={val_psnr_bic:.2f} dB | \"\n",
    "          f\"lr={scheduler.get_last_lr()[0]:.1e} | {dt:.1f}s\")\n",
    "\n",
    "print(f\"Best val PSNR(Y): {best_val_psnr:.2f} dB  (saved → {CKPT_PATH})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01896edb-eeff-4d44-8b6c-0875513914de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
