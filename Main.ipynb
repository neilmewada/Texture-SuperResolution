{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8a1209b-0c2b-42a6-95e5-4a986a37a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04051e60-8c70-4318-b83b-8fecf80b5afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Projects\\\\Texture-SuperResolution/ambient-cg-images'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to your DTD dataset (downloaded and extracted)\n",
    "\n",
    "DATASET_PATH = f\"{os.getcwd()}/ambient-cg-images\"\n",
    "DATASET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7cbf2e-ef82-42c4-9d24-bb7c451937d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neelr\\AppData\\Roaming\\Python\\Python313\\site-packages\\PIL\\Image.py:3442: DecompressionBombWarning: Image size (150994944 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (101, 3) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     52\u001b[39m     X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=test_size, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_val, Y_train, Y_val\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m X_train, X_val, Y_train, Y_val = \u001b[43mload_dtd_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m192\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mload_dtd_dataset\u001b[39m\u001b[34m(root_dir, scale_factor, patch_size, test_size)\u001b[39m\n\u001b[32m     45\u001b[39m     X.append(to_tensor(lr_img).numpy())\n\u001b[32m     46\u001b[39m     Y.append(to_tensor(hr_img).numpy())\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m X = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m Y = np.array(Y, dtype=np.float32)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Train/val split\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (101, 3) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# ðŸ”¹ Preprocessing Function\n",
    "# -----------------------------\n",
    "IMAGE_LIMIT = 100\n",
    "\n",
    "def load_dtd_dataset(root_dir, scale_factor=2, patch_size=64, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Loads DTD dataset and creates LR-HR pairs as numpy arrays.\n",
    "    \"\"\"\n",
    "    image_files = []\n",
    "    counter = 0\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        if counter > IMAGE_LIMIT:\n",
    "            break\n",
    "        for f in files:\n",
    "            if counter > IMAGE_LIMIT:\n",
    "                break\n",
    "            if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_files.append(os.path.join(root, f))\n",
    "                counter += 1\n",
    "                \n",
    "    to_tensor = transforms.ToTensor()\n",
    "    X, Y = [], []\n",
    "\n",
    "    for img_path in image_files:\n",
    "        hr_img = Image.open(img_path).convert(\"RGB\")\n",
    "        width, height = hr_img.size\n",
    "        patch_size = min(width, height)\n",
    "\n",
    "        # Ensure patch fits\n",
    "        if width < patch_size or height < patch_size:\n",
    "            hr_img = hr_img.resize((patch_size, patch_size), Image.BICUBIC)\n",
    "        else:\n",
    "            # Random crop\n",
    "            left = np.random.randint(0, width - patch_size + 1)\n",
    "            top = np.random.randint(0, height - patch_size + 1)\n",
    "            hr_img = hr_img.crop((left, top, left + patch_size, top + patch_size))\n",
    "\n",
    "        # Create low-res version\n",
    "        lr_size = (patch_size // scale_factor, patch_size // scale_factor)\n",
    "        lr_img = hr_img.resize(lr_size, Image.BICUBIC)\n",
    "        lr_img = lr_img.resize((patch_size, patch_size), Image.BICUBIC)\n",
    "\n",
    "        # Convert to tensors (C,H,W) and append\n",
    "        X.append(to_tensor(lr_img).numpy())\n",
    "        Y.append(to_tensor(hr_img).numpy())\n",
    "\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    Y = np.array(Y, dtype=np.float32)\n",
    "\n",
    "    # Train/val split\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=test_size, random_state=42)\n",
    "\n",
    "    return X_train, X_val, Y_train, Y_val\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = load_dtd_dataset(DATASET_PATH, scale_factor=2, patch_size=192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cc8f5-a170-4f30-9f68-34308cbf65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextureDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X)  # convert to torch.Tensor\n",
    "        self.Y = torch.from_numpy(Y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.Y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1103d00-2679-4b76-8419-12b83ae2c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextureDataset(X_train, Y_train)\n",
    "val_dataset = TextureDataset(X_val, Y_val)\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Val samples:\", len(val_dataset))\n",
    "\n",
    "# Test one sample\n",
    "lr, hr = train_dataset[56]\n",
    "print(\"LR shape:\", lr.shape, \"HR shape:\", hr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0f5496-4c83-4e24-b0cf-24da4433c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(np.transpose(lr.numpy(), (1, 2, 0)))\n",
    "axes[0].set_title(\"LR (bicubic upsampled)\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(np.transpose(hr.numpy(), (1, 2, 0)))\n",
    "axes[1].set_title(\"HR (ground truth patch)\")\n",
    "axes[1].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa60c7a-fdc8-4e46-8d81-7aa8ff47b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE   = 16\n",
    "NUM_EPOCHS   = 400\n",
    "LR           = 1e-3\n",
    "STEP_EVERY   = 15\n",
    "GAMMA        = 0.5\n",
    "NUM_WORKERS  = 0\n",
    "PIN_MEMORY   = False\n",
    "FSRCNN_SCALE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae8d6f9-a9a6-403f-96aa-dfeefa21e3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f4cbf0-228c-4089-b9f0-6fc7f4d3f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SRCNN\n",
    "from model import FSRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d3a49c-bcaf-4276-9f00-2d281f6ca808",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def convert_rgb_to_y(img):\n",
    "    if type(img) == np.ndarray:\n",
    "        return 16. + (64.738 * img[:, :, 0] + 129.057 * img[:, :, 1] + 25.064 * img[:, :, 2]) / 256.\n",
    "    elif type(img) == torch.Tensor:\n",
    "        if len(img.shape) == 4:\n",
    "            img = img.squeeze(0)\n",
    "        return 16. + (64.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) / 256.\n",
    "    else:\n",
    "        raise Exception('Unknown Type', type(img))\n",
    "\n",
    "\n",
    "def convert_rgb_to_ycbcr(img):\n",
    "    if type(img) == np.ndarray:\n",
    "        y = 16. + (64.738 * img[:, :, 0] + 129.057 * img[:, :, 1] + 25.064 * img[:, :, 2]) / 256.\n",
    "        cb = 128. + (-37.945 * img[:, :, 0] - 74.494 * img[:, :, 1] + 112.439 * img[:, :, 2]) / 256.\n",
    "        cr = 128. + (112.439 * img[:, :, 0] - 94.154 * img[:, :, 1] - 18.285 * img[:, :, 2]) / 256.\n",
    "        return np.array([y, cb, cr]).transpose([1, 2, 0])\n",
    "    elif type(img) == torch.Tensor:\n",
    "        if len(img.shape) == 4:\n",
    "            img = img.squeeze(0)\n",
    "        y = 16. + (64.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) / 256.\n",
    "        cb = 128. + (-37.945 * img[0, :, :] - 74.494 * img[1, :, :] + 112.439 * img[2, :, :]) / 256.\n",
    "        cr = 128. + (112.439 * img[0, :, :] - 94.154 * img[1, :, :] - 18.285 * img[2, :, :]) / 256.\n",
    "        return torch.cat([y, cb, cr], 0).permute(1, 2, 0)\n",
    "    else:\n",
    "        raise Exception('Unknown Type', type(img))\n",
    "\n",
    "\n",
    "def convert_ycbcr_to_rgb(img):\n",
    "    if type(img) == np.ndarray:\n",
    "        r = 298.082 * img[:, :, 0] / 256. + 408.583 * img[:, :, 2] / 256. - 222.921\n",
    "        g = 298.082 * img[:, :, 0] / 256. - 100.291 * img[:, :, 1] / 256. - 208.120 * img[:, :, 2] / 256. + 135.576\n",
    "        b = 298.082 * img[:, :, 0] / 256. + 516.412 * img[:, :, 1] / 256. - 276.836\n",
    "        return np.array([r, g, b]).transpose([1, 2, 0])\n",
    "    elif type(img) == torch.Tensor:\n",
    "        if len(img.shape) == 4:\n",
    "            img = img.squeeze(0)\n",
    "        r = 298.082 * img[0, :, :] / 256. + 408.583 * img[2, :, :] / 256. - 222.921\n",
    "        g = 298.082 * img[0, :, :] / 256. - 100.291 * img[1, :, :] / 256. - 208.120 * img[2, :, :] / 256. + 135.576\n",
    "        b = 298.082 * img[0, :, :] / 256. + 516.412 * img[1, :, :] / 256. - 276.836\n",
    "        return torch.cat([r, g, b], 0).permute(1, 2, 0)\n",
    "    else:\n",
    "        raise Exception('Unknown Type', type(img))\n",
    "\n",
    "\n",
    "def calc_psnr(img1, img2):\n",
    "    return 10. * torch.log10(1. / torch.mean((img1 - img2) ** 2))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "model = FSRCNN(FSRCNN_SCALE, num_channels=3).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam([\n",
    "        {'params': model.first_part.parameters()},\n",
    "        {'params': model.mid_part.parameters()},\n",
    "        {'params': model.last_part.parameters(), 'lr': LR * 0.1}\n",
    "    ], LR)\n",
    "\n",
    "best_weights = copy.deepcopy(model.state_dict())\n",
    "best_epoch = 0\n",
    "best_psnr = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_losses = AverageMeter()\n",
    "\n",
    "    with tqdm(total=(len(train_dataset) - len(train_dataset) % BATCH_SIZE)) as t:\n",
    "        t.set_description('epoch: {}/{}'.format(epoch, NUM_EPOCHS - 1))\n",
    "\n",
    "        for data in train_loader:\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            preds = model(inputs)\n",
    "\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            epoch_losses.update(loss.item(), len(inputs))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n",
    "            t.update(len(inputs))\n",
    "\n",
    "    model.eval()\n",
    "    epoch_psnr = AverageMeter()\n",
    "\n",
    "    for data in val_loader:\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(inputs).clamp(0.0, 1.0)\n",
    "\n",
    "        epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n",
    "\n",
    "    print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n",
    "\n",
    "    if epoch_psnr.avg > best_psnr:\n",
    "        best_epoch = epoch\n",
    "        best_psnr = epoch_psnr.avg\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "print('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\n",
    "torch.save(best_weights, os.path.join(\"./\", 'best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdddbe6c-366d-41f9-8040-3a70e7b975e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_hwc_np(t: torch.Tensor):\n",
    "    \"\"\"[C,H,W] float in [0,1] -> HxW(c) numpy for imshow.\"\"\"\n",
    "    t = t.detach().cpu().clamp(0,1)\n",
    "    if t.ndim == 3 and t.shape[0] == 3:\n",
    "        return np.transpose(t.numpy(), (1, 2, 0))\n",
    "    return t.squeeze(0).numpy()\n",
    "\n",
    "def psnr(a: torch.Tensor, b: torch.Tensor, eps=1e-10):\n",
    "    # a,b: [C,H,W] in [0,1]\n",
    "    mse = torch.mean((a - b) ** 2).item()\n",
    "    return 20 * np.log10(1.0 / math.sqrt(mse + eps))\n",
    "\n",
    "def rgb_to_y(t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"[C,H,W] RGB in [0,1] -> [1,H,W] Y (BT.601).\"\"\"\n",
    "    if t.shape[0] == 1:\n",
    "        return t\n",
    "    r, g, b = t[0], t[1], t[2]\n",
    "    y = 0.299 * r + 0.587 * g + 0.114 * b\n",
    "    return y.unsqueeze(0)\n",
    "\n",
    "def rgb_to_ycbcr(t: torch.Tensor):\n",
    "    \"\"\"[C,H,W] RGB in [0,1] -> (Y,Cb,Cr) each [1,H,W], BT.601 with 0.5 offset for Cb/Cr.\"\"\"\n",
    "    r, g, b = t[0], t[1], t[2]\n",
    "    Y  = 0.299  * r + 0.587  * g + 0.114  * b\n",
    "    Cb = -0.168736 * r - 0.331264 * g + 0.5      * b + 0.5\n",
    "    Cr =  0.5      * r - 0.418688 * g - 0.081312 * b + 0.5\n",
    "    return Y.unsqueeze(0), Cb.unsqueeze(0), Cr.unsqueeze(0)\n",
    "\n",
    "def ycbcr_to_rgb(Y: torch.Tensor, Cb: torch.Tensor, Cr: torch.Tensor):\n",
    "    \"\"\"Y,Cb,Cr in [0,1] (Cb/Cr centered at 0.5) -> [3,H,W] RGB in [0,1].\"\"\"\n",
    "    r = Y + 1.402   * (Cr - 0.5)\n",
    "    g = Y - 0.344136 * (Cb - 0.5) - 0.714136 * (Cr - 0.5)\n",
    "    b = Y + 1.772   * (Cb - 0.5)\n",
    "    out = torch.stack([r, g, b], dim=0).clamp(0, 1)\n",
    "    return out\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ---------- Pick random samples from the validation set ----------\n",
    "NUM_SAMPLES = 4\n",
    "rand_idxs = random.sample(range(len(train_dataset)), k=min(NUM_SAMPLES, len(train_dataset)))\n",
    "\n",
    "for i, idx in enumerate(rand_idxs, 1):\n",
    "    lr, hr = train_dataset[idx]   # [C,H,W] in [0,1]; LR is bicubic-upsampled to HR size\n",
    "\n",
    "    # Match model channels\n",
    "    if model.conv1.in_channels == 1:\n",
    "        # Y-channel model\n",
    "        if lr.shape[0] == 3:\n",
    "            # Keep chroma from LR for colorizing the SR output\n",
    "            Y_lr, Cb_lr, Cr_lr = rgb_to_ycbcr(lr)\n",
    "            Y_hr = rgb_to_y(hr)\n",
    "            lr_in = Y_lr.to(device)                     # [1,H,W]\n",
    "            with torch.no_grad():\n",
    "                sr_y = model(lr_in.unsqueeze(0)).squeeze(0).cpu().clamp(0,1)  # [1,H,W]\n",
    "            # Recolor SR using LR chroma for display\n",
    "            sr_rgb = ycbcr_to_rgb(sr_y, Cb_lr, Cr_lr)\n",
    "            # For fair PSNR, compute on Y\n",
    "            psnr_bic = psnr(Y_lr, Y_hr)\n",
    "            psnr_sr  = psnr(sr_y, Y_hr)\n",
    "\n",
    "            # What to show:\n",
    "            img_lr = lr                    # color bicubic input (for context)\n",
    "            img_sr = sr_rgb                # colorized SR\n",
    "            img_hr = hr                    # GT color\n",
    "\n",
    "            title_mid = f\"SRCNN (Y only)  PSNR(Y)={psnr_sr:.2f} dB\"\n",
    "            title_left = f\"LR Bicubic  PSNR(Y)={psnr_bic:.2f} dB\"\n",
    "        else:\n",
    "            # Dataset already single-channel\n",
    "            lr_in = lr.to(device)\n",
    "            with torch.no_grad():\n",
    "                sr = model(lr_in.unsqueeze(0)).squeeze(0).cpu().clamp(0,1)\n",
    "            psnr_bic = psnr(lr, hr)\n",
    "            psnr_sr  = psnr(sr, hr)\n",
    "            img_lr, img_sr, img_hr = lr, sr, hr\n",
    "            title_mid = f\"SRCNN  PSNR={psnr_sr:.2f} dB\"\n",
    "            title_left = f\"LR Bicubic  PSNR={psnr_bic:.2f} dB\"\n",
    "    else:\n",
    "        # RGB model\n",
    "        lr_in = lr.to(device)\n",
    "        with torch.no_grad():\n",
    "            sr = model(lr_in.unsqueeze(0)).squeeze(0).cpu().clamp(0,1)\n",
    "        # PSNR on Y for fair comparison (common in SR)\n",
    "        psnr_bic = psnr(rgb_to_y(lr), rgb_to_y(hr))\n",
    "        psnr_sr  = psnr(rgb_to_y(sr), rgb_to_y(hr))\n",
    "        img_lr, img_sr, img_hr = lr, sr, hr\n",
    "        title_mid = f\"SRCNN (RGB)  PSNR(Y)={psnr_sr:.2f} dB\"\n",
    "        title_left = f\"LR Bicubic  PSNR(Y)={psnr_bic:.2f} dB\"\n",
    "\n",
    "    # ---------- Plot ----------\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "    axes[0].imshow(to_hwc_np(img_lr))\n",
    "    axes[0].set_title(title_left)\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(to_hwc_np(img_sr))\n",
    "    axes[1].set_title(title_mid)\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(to_hwc_np(img_hr))\n",
    "    axes[2].set_title(\"HR Ground Truth\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ece342-a3e4-47e8-9cb1-4dbbdefada32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
